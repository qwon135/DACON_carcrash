{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9651e-e42f-4b43-a52c-54d8b6fb58bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from einops import rearrange\n",
    "from decord import VideoReader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "from transformers import AutoModel, AutoImageProcessor, AutoConfig\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorchvideo.transforms.transforms_factory import create_video_transform\n",
    "\n",
    "from crash_modules.crash_dataset import VideoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4247e-f74d-4323-8005-242038d992a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\":2023,\n",
    "    \"model_name\":\"facebook/timesformer-base-finetuned-k600\",\n",
    "    \"batch_size\":3,\n",
    "    \"learning_rate\":1e-5,\n",
    "    \"data_dir\":'',\n",
    "    \"checkpoint_dir\":'./checkpoint_crashego16',\n",
    "    \"submission_dir\":'./submission',\n",
    "    \"n_classes\": 3,\n",
    "    \"label_dict\":{\n",
    "         1:0,\n",
    "         2:0,\n",
    "         3:0,\n",
    "         4:0,\n",
    "         5:0,\n",
    "         6:0,\n",
    "         7:1,\n",
    "         8:1,\n",
    "         9:1,\n",
    "        10:1,\n",
    "        11:1,\n",
    "        12:1,\n",
    "\n",
    "        13:2,\n",
    "        14:2,\n",
    "        15:2,\n",
    "        16:2,\n",
    "        17:2,\n",
    "        18:2,\n",
    "\n",
    "    },\n",
    "    \"label_reverse_dict\":{\n",
    "        0:0,\n",
    "        1:1,\n",
    "        2:2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7430352-4e4d-414b-a180-ec554b3c9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1692b-89ab-4739-a668-a191d43d85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f\"{config['data_dir']}/train_ver2.csv\")\n",
    "train_df = train_df[['sample_id', 'video_path', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04928136-a408-4278-acbb-6284073c853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sample_id'] = train_df['sample_id'].apply(lambda x: int(x.split('_')[1]))\n",
    "train_df['video_path'] = train_df['video_path'].apply(lambda x: config['data_dir'] + x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba29cd0-c1da-43ab-a68c-b09688c70576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label_split'] = train_df['label'].apply(config['label_dict'].get)\n",
    "train_df['label'] = train_df['label_split']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84d5626-6b8b-4ea9-81a8-3320337d93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AutoConfig.from_pretrained('facebook/timesformer-base-finetuned-k600')\n",
    "image_processor_config = AutoImageProcessor.from_pretrained('facebook/timesformer-base-finetuned-k600')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612a650-4c4d-4f1a-a7f7-11c22ff6b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = create_video_transform(\n",
    "    mode='train',\n",
    "    num_samples=16,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")\n",
    "\n",
    "val_transform = create_video_transform(\n",
    "    mode='val',\n",
    "    num_samples=16,\n",
    "    video_mean = tuple(image_processor_config.image_mean),\n",
    "    video_std = tuple(image_processor_config.image_std),\n",
    "    crop_size = tuple(image_processor_config.crop_size.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69747543-ce8b-4f0c-aa58-de46c616490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLVideoModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.model = AutoModel.from_pretrained('facebook/timesformer-base-finetuned-k600')\n",
    "        self.classifier = nn.LazyLinear(3)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x).last_hidden_state.mean(dim=1)\n",
    "        x_out = self.classifier(x)\n",
    "        return x_out\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        loss = self.loss(y_hats, batch[\"label\"])\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        video, label, label_split = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss = self.loss(y_hats, batch[\"label\"])\n",
    "\n",
    "        self.log(\"valid_loss\", loss)\n",
    "\n",
    "        step_output = [y_hats, label]\n",
    "        return step_output\n",
    "    \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        video, _, _ = batch['video'], batch['label'], batch['label_split']\n",
    "        y_hats = self.forward(batch[\"video\"])\n",
    "        step_output = y_hats\n",
    "        return step_output\n",
    "\n",
    "    def validation_epoch_end(self, step_outputs):\n",
    "        preds = []\n",
    "        labels = []\n",
    "\n",
    "        for step_output in step_outputs:\n",
    "            pred, label = step_output\n",
    "            preds += pred.argmax(1).detach().cpu().tolist()\n",
    "            labels += label.tolist()            \n",
    "\n",
    "        score = f1_score(labels, preds, average='macro')\n",
    "        self.log(\"val_score\", score)\n",
    "        return score\n",
    "    \n",
    "    def post_preproc(self, step_outputs):\n",
    "        preds = []\n",
    "        for step_output in step_outputs:\n",
    "            pred = step_output[0]\n",
    "            preds += pred.argmax(1).detach().cpu().tolist()            \n",
    "        \n",
    "        return preds\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        opt1 = torch.optim.Adam(self.parameters(), lr=self.learning_rate)        \n",
    "        opt2 = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        optimizers = [opt1, opt2]\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt1, mode='max', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)        \n",
    "        lr_schedulers = {\"scheduler\": scheduler, \"monitor\": \"valid_loss\"}\n",
    "        \n",
    "        return optimizers, lr_schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5dbf9-b7f7-404b-9843-faae40e7eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=2023)\n",
    "k = 1\n",
    "for t_idx, v_idx in kf.split(X = train_df['sample_id'], y = train_df['label'] ):\n",
    "    train_df_for_dataset = train_df.loc[t_idx].reset_index(drop=True).values\n",
    "    val_df_for_dataset = train_df.loc[v_idx].reset_index(drop=True).values\n",
    "\n",
    "    train_dataset = VideoDataset(train_df_for_dataset, transform=train_transform, mode='train')\n",
    "    val_dataset = VideoDataset(val_df_for_dataset, transform=val_transform, mode = 'valid')\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size= config['batch_size'],  num_workers= 8, pin_memory= True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size = config['batch_size']*2, num_workers= 8, pin_memory= True)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_score',\n",
    "        dirpath=config['checkpoint_dir'],\n",
    "        filename=f'{config[\"model_name\"]}'+'-{epoch:02d}-{train_loss:.4f}-{valid_loss:.4f}-{val_score:.4f}',\n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_score\",\n",
    "        patience=3,\n",
    "        verbose=False,\n",
    "        mode=\"max\")\n",
    "\n",
    "    pl_video_model = PLVideoModel(config)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='auto', \n",
    "        precision=16,\n",
    "        callbacks=[early_stop_callback, checkpoint_callback],                                \n",
    "    )\n",
    "    trainer.fit(pl_video_model, train_dataloader, val_dataloader)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3529639a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
